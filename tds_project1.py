# -*- coding: utf-8 -*-
"""TDS_Project1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZpOrr6RSTaMVrfIiy_ni9kxyJOvWSJwJ
"""

!pip install pandas requests

import pandas as pd

# Load users.csv from your GitHub repository
users_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/users.csv'  # Update with your GitHub URL

users_df = pd.read_csv(users_url)

# Get the top 5 users by followers
top_users = users_df.nlargest(5, 'followers')

# Get their logins in order
top_user_logins = top_users['login'].tolist()

# Print the logins comma-separated
print(", ".join(top_user_logins))

import pandas as pd

# Load users.csv from your GitHub repository
users_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/users.csv'

users_df = pd.read_csv(users_url)

# Convert created_at to datetime
users_df['created_at'] = pd.to_datetime(users_df['created_at'])

# Get the 5 earliest registered users
earliest_users = users_df.nsmallest(5, 'created_at')

# Get their logins in order
earliest_user_logins = earliest_users['login'].tolist()

# Print the logins comma-separated
print(", ".join(earliest_user_logins))

import pandas as pd

# Load repositories.csv from your GitHub repository
repositories_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/repositories.csv'  # Update with your GitHub URL

repositories_df = pd.read_csv(repositories_url)

# Filter out missing licenses
valid_licenses = repositories_df[repositories_df['license_name'] != '']

# Count occurrences of each license
license_counts = valid_licenses['license_name'].value_counts()

# Get the top 3 most popular licenses
top_licenses = license_counts.head(3)

# Get the license names in order
popular_license_names = top_licenses.index.tolist()

# Print the license names comma-separated
print(", ".join(popular_license_names))

import pandas as pd

# Load users.csv from your GitHub repository
users_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/users.csv'  # Update with your GitHub URL

users_df = pd.read_csv(users_url)

# Filter out missing company names and convert to uppercase for consistency
users_df['company'] = users_df['company'].str.strip().str.upper()
valid_companies = users_df[users_df['company'] != '']

# Count occurrences of each company
company_counts = valid_companies['company'].value_counts()

# Get the company with the highest count
most_common_company = company_counts.idxmax()
most_common_count = company_counts.max()

# Print the result
print(f"The majority of developers work at: {most_common_company} (Count: {most_common_count})")

import pandas as pd

# Load repositories.csv from your GitHub repository
repositories_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/repositories.csv'  # Update with your GitHub URL

repositories_df = pd.read_csv(repositories_url)

# Filter out missing language entries
valid_languages = repositories_df[repositories_df['language'] != '']

# Count occurrences of each programming language
language_counts = valid_languages['language'].value_counts()

# Get the most popular language
most_popular_language = language_counts.idxmax()
most_popular_count = language_counts.max()

# Print the result
print(f"The most popular programming language is: {most_popular_language} (Count: {most_popular_count})")

import pandas as pd

# Load users.csv from your GitHub repository
users_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/users.csv'  # Update with your GitHub URL

# Load repositories.csv from your GitHub repository
repositories_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/repositories.csv'  # Update with your GitHub URL

users_df = pd.read_csv(users_url)
repositories_df = pd.read_csv(repositories_url)

# Convert created_at to datetime
users_df['created_at'] = pd.to_datetime(users_df['created_at'])

# Filter users who joined after 2020
recent_users = users_df[users_df['created_at'] > '2020-01-01']

# Filter repositories of recent users
recent_user_logins = recent_users['login'].tolist()
recent_repositories = repositories_df[repositories_df['login'].isin(recent_user_logins)]

# Filter out missing language entries
valid_languages = recent_repositories[recent_repositories['language'] != '']

# Count occurrences of each programming language
language_counts = valid_languages['language'].value_counts()

# Get the second most popular language
second_most_popular_language = language_counts.nlargest(2).index[1]
second_most_popular_count = language_counts.nlargest(2).values[1]

# Print the result
print(f"The second most popular programming language among users who joined after 2020 is: {second_most_popular_language} (Count: {second_most_popular_count})")

import pandas as pd

# Load repositories.csv from your GitHub repository
repositories_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/repositories.csv'  # Update with your GitHub URL

repositories_df = pd.read_csv(repositories_url)

# Filter out missing language entries and star counts
valid_repositories = repositories_df[
    (repositories_df['language'] != '') &
    (repositories_df['stargazers_count'].notnull())
]

# Group by language and calculate average stars
average_stars = valid_repositories.groupby('language')['stargazers_count'].mean()

# Get the language with the highest average stars
highest_average_language = average_stars.idxmax()
highest_average_count = average_stars.max()

# Print the result
print(f"The programming language with the highest average number of stars per repository is: {highest_average_language} (Average Stars: {highest_average_count})")

import pandas as pd

# Load users.csv from your GitHub repository
users_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/users.csv'  # Update with your GitHub URL

users_df = pd.read_csv(users_url)

# Calculate leader_strength
users_df['leader_strength'] = users_df['followers'] / (1 + users_df['following'])

# Get the top 5 users by leader_strength
top_leaders = users_df.nlargest(5, 'leader_strength')

# Get their logins in order
top_leader_logins = top_leaders['login'].tolist()

# Print the logins comma-separated
print(", ".join(top_leader_logins))

import pandas as pd

# Load users.csv from your GitHub repository
users_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/users.csv'  # Update with your GitHub URL

users_df = pd.read_csv(users_url)

# Calculate correlation between followers and public_repos
correlation = users_df['followers'].corr(users_df['public_repos'])

# Print the correlation rounded to 3 decimal places
print(f"Correlation between followers and public repositories: {correlation:.3f}")

import pandas as pd
import statsmodels.api as sm

# Load users.csv from your GitHub repository
users_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/users.csv'  # Update with your GitHub URL

users_df = pd.read_csv(users_url)

# Prepare the data for regression
X = users_df['public_repos']  # Independent variable
y = users_df['followers']      # Dependent variable

# Add a constant to the independent variable
X = sm.add_constant(X)

# Fit the regression model
model = sm.OLS(y, X).fit()

# Get the regression results
intercept = model.params[0]
slope = model.params[1]

# Print the results
print(f"Regression results:")
print(f"Intercept: {intercept:.2f}")
print(f"Slope (additional followers per repo): {slope:.2f}")
print(f"R-squared: {model.rsquared:.3f}")

# Additional followers per additional repository
additional_followers_per_repo = slope
print(f"Estimated additional followers per additional public repository: {additional_followers_per_repo:.2f}")

import pandas as pd

# Load repositories.csv from your GitHub repository
repos_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/repositories.csv'  # Update with your GitHub URL

# Read the CSV file
repos_df = pd.read_csv(repos_url)

# Check for the relevant columns
print(repos_df.columns)

# Create binary indicators for projects and wikis
repos_df['projects_enabled'] = repos_df['has_projects'].astype(int)  # Convert to binary (1 for True, 0 for False)
repos_df['wiki_enabled'] = repos_df['has_wiki'].astype(int)  # Convert to binary (1 for True, 0 for False)

# Calculate the correlation between projects and wiki enabled
correlation = repos_df['projects_enabled'].corr(repos_df['wiki_enabled'])

# Print the correlation rounded to 3 decimal places
print(f"Correlation between projects enabled and wiki enabled: {correlation:.3f}")

import pandas as pd

# Load users.csv from your GitHub repository
users_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/users.csv'  # Update with your GitHub URL

# Read the CSV file
users_df = pd.read_csv(users_url)

# Calculate the average following for hireable users
hireable_avg_following = users_df[users_df['hireable'] == True]['following'].mean()

# Calculate the average following for non-hireable users
non_hireable_avg_following = users_df[users_df['hireable'] == False]['following'].mean()

# Calculate the difference
difference = hireable_avg_following - non_hireable_avg_following

# Print the result rounded to 3 decimal places
print(f"Average following for hireable users minus non-hireable users: {difference:.3f}")

import pandas as pd
import numpy as np
import statsmodels.api as sm

# Load users.csv from your GitHub repository
users_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/users.csv'  # Update with your GitHub URL

# Read the CSV file
users_df = pd.read_csv(users_url)

# Filter out users without bios
users_with_bios = users_df[users_df['bio'].notna()]

# Calculate the bio word count
users_with_bios['bio_word_count'] = users_with_bios['bio'].str.split().str.len()

# Prepare the independent (X) and dependent (y) variables
X = users_with_bios['bio_word_count']
y = users_with_bios['followers']

# Add a constant to the independent variable (for the intercept)
X = sm.add_constant(X)

# Perform linear regression
model = sm.OLS(y, X).fit()

# Get the slope (coefficient) for bio_word_count
slope = model.params['bio_word_count']

# Print the result rounded to 3 decimal places
print(f"Regression slope of followers on bio word count: {slope:.3f}")

import pandas as pd

# Load repositories.csv from your GitHub repository
repos_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/repositories.csv'  # Update with your GitHub URL

# Read the CSV file
repos_df = pd.read_csv(repos_url)

# Convert 'created_at' to datetime
repos_df['created_at'] = pd.to_datetime(repos_df['created_at'])

# Filter for repositories created on weekends (Saturday and Sunday)
repos_df['weekday'] = repos_df['created_at'].dt.day_name()  # Get the name of the weekday
weekend_repos = repos_df[repos_df['weekday'].isin(['Saturday', 'Sunday'])]

# Count the number of repositories created by each user
repo_count = weekend_repos['login'].value_counts()

# Get the top 5 users
top_5_users = repo_count.head(5).index.tolist()

# Print the top 5 users' logins in order, comma-separated
print("Top 5 users who created the most repositories on weekends:", ', '.join(top_5_users))

import pandas as pd

# Load users.csv from your GitHub repository
users_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/users.csv'  # Update with your GitHub URL

# Read the CSV file
users_df = pd.read_csv(users_url)

# Calculate the fraction of users with email for hireable users
hireable_with_email = users_df[users_df['hireable'] == True]['email'].notna().mean()

# Calculate the fraction of users with email for non-hireable users
non_hireable_with_email = users_df[users_df['hireable'] == False]['email'].notna().mean()

# Calculate the difference
email_difference = hireable_with_email - non_hireable_with_email

# Print the result rounded to 3 decimal places
print(f"Fraction of users with email when hireable=true minus for the rest: {email_difference:.3f}")

import pandas as pd

# Load users.csv from your GitHub repository
users_url = 'https://raw.githubusercontent.com/Alizalily/TDS_Project1/main/users.csv'  # Update with your GitHub URL

# Read the CSV file
users_df = pd.read_csv(users_url)

# Filter out users without names
users_df = users_df[users_df['name'].notna()]

# Extract surnames by trimming and splitting names
users_df['surname'] = users_df['name'].str.strip().str.split().str[-1]

# Count occurrences of each surname
surname_counts = users_df['surname'].value_counts()

# Identify the maximum count
max_count = surname_counts.max()

# Get the most common surnames (handling ties)
most_common_surnames = surname_counts[surname_counts == max_count].index.tolist()

# Sort surnames alphabetically
most_common_surnames.sort()

# Print the result as a comma-separated string
print("Most common surnames:", ', '.join(most_common_surnames))